트랜스포머 모델은 자연어 처리에서 혁신적인 성과를 거둔 모델로, 특히 문장 내 단어들 간의 관계를 잘 이해하는 데 강점을 가지고 있습니다. 트랜스포머의 기본 구성 요소 중 하나인 인코더(Encoder)는 여러 층으로 이루어져 있으며, 각 층은 주로 셀프 어텐션(Self-Attention) 메커니즘과 피드 포워드(Feed-Forward) 신경망으로 구성됩니다. 

### 트랜스포머 인코더의 구성 요소

#### 1. 셀프 어텐션 (Self-Attention) 메커니즘

셀프 어텐션 메커니즘은 입력 시퀀스 내의 모든 위치가 다른 모든 위치와 상호작용하며, 각 단어가 문맥 내에서 얼마나 중요한지 결정합니다. 셀프 어텐션의 주요 목적은 문장 내 단어들 간의 관계를 학습하는 것입니다.

##### 셀프 어텐션의 계산 과정

1. **쿼리, 키, 값 벡터 생성**:
   - 입력 벡터 \( X \)는 쿼리 \( Q \), 키 \( K \), 값 \( V \) 벡터로 변환됩니다. 이때 사용되는 가중치 행렬 \( W_Q, W_K, W_V \)는 학습 가능한 파라미터입니다.
     \[
     Q = XW_Q, \quad K = XW_K, \quad V = XW_V
     \]

2. **어텐션 스코어 계산**:
   - 각 쿼리와 모든 키 간의 유사도를 계산하여 어텐션 스코어 \( e \)를 구합니다. 이는 점곱을 통해 계산됩니다.
     \[
     e_{ij} = \frac{(Q_i \cdot K_j)}{\sqrt{d_k}}
     \]
   여기서 \( d_k \)는 키 벡터의 차원입니다. 이 스케일링은 더 안정적인 그래디언트 학습을 위해 필요합니다.

3. **어텐션 가중치 계산**:
   - 어텐션 스코어에 소프트맥스 함수를 적용하여 가중치 \( a \)를 구합니다.
     \[
     a_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{n} \exp(e_{ik})}
     \]

4. **컨텍스트 벡터 계산**:
   - 각 값 벡터 \( V \)에 어텐션 가중치를 곱하여 가중합을 구합니다. 이 가중합이 컨텍스트 벡터입니다.
     \[
     \text{context}_i = \sum_{j=1}^{n} a_{ij} V_j
     \]
